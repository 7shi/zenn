---
title: "Tanuki で MoE の挙動を調査"
emoji: "🦝"
type: "tech"
topics:
  - "llm"
  - "moe"
published: true
published_at: "2025-02-21 07:08"
---

[Tanuki-8B-dpo-v1.0](https://huggingface.co/weblab-GENIAC/Tanuki-8B-dpo-v1.0) と [Tanuki-8x8B-dpo-v1.0](https://huggingface.co/weblab-GENIAC/Tanuki-8x8B-dpo-v1.0) を使って、Ollama での MoE の挙動を調査します。

# MoE

MoE（Mixture of Experts）は、大規模言語モデル（LLM）の一形態です。モデルの中にエキスパート（専門家）を複数用意して、質問や文章の一部ごとに最適なエキスパートを選んで回答を生成します。

例えば、数学が得意な専門家や、歴史が得意な専門家がいて、必要なときだけその専門家が働くイメージです。全部の専門家を一度に使うわけではないため、モデルサイズに比べて計算は軽くなります。

最近話題になった DeepSeek も MoE です。DeepSeek-V3 は全体で 671B ありますが、256 個のエキスパートで構成されており、同時に 8 つのエキスパート (37B) がアクティブになります。👉[参考](https://7shi.hateblo.jp/entry/2025/01/08/000133)

DeepSeek-V3 は手元のマシンで動かすには巨大すぎるため、本記事では Tanuki-8x8B-dpo-v1.0 を使用します。（8x がエキスパートの個数）

# 環境

以下の環境で測定しました。

- OS: Windows 11 Home [Version 10.0.26100.3194]
- RAM: 64GB (DDR4-3200)
- CPU: AMD Ryzen 5 5600X 6-Core Processor
- GPU: AMD Radeon RX 7600 XT (VRAM 16GB)
- Ollama: 0.5.11

:::message
この Ollama のバージョンでは、VRAM の容量を見て自動的に一部を GPU にオフロードします。
:::

`/set verbose` を指定して、以下のプロンプトを実行しました。

```text
AIの未来を予測してください。（100字）
```

# Tanuki-8B-dpo-v1.0

まず比較対象として、MoE ではない 8B で測定します。

```sh
ollama run 7shi/tanuki-dpo-v1.0
```

パラ数|量子化|サイズ|環境|tps|RAM|VRAM
---:|---|---:|---|---:|---:|---:
8B|Q6_K|6.2GB|CPU|6.77|6.8GB|
8B|Q6_K|6.2GB|GPU|37.50|6.2GB|7.9GB

:::message
RAM/VRAM 列は、測定前との差分で算出したメモリ使用量です。サイズは量子化後のモデルサイズです。tps (tokens per second) は 1 秒あたりの出力トークン数です。
:::

:::details 出力例
AIの未来は、より高度で人間らしい知能を持つ「汎用人工知能」や「超知能」が実現し、医療、教育、交通、エンターテインメントなど多岐にわたる分野で革新をもたらすことが期待されます。また、倫理的課題やプライバシー保護、労働市場への影響も重要なテーマとなり、人間とAIの共生が進むでしょう。さらに、自律型ロボットやスマートシティの実現により、生活の質が向上する一方で、新たな社会問題も生じる可能性があります。

----

AIの未来は、高度な自動化と人間との協働が進むでしょう。医療や教育、エンターテインメントなど多岐にわたる分野で革新が期待され、倫理的課題も解決に向けた議論が深まります。透明性とプライバシー保護が重要となり、持続可能な開発が求められます。
:::

# Tanuki-8x8B-dpo-v1.0

本来の目的である MoE モデルを測定します。

## Q4_K_M

まず一般的に使われる Q4_K_M で量子化したモデルです。

```sh
ollama run hf.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GGUF
```

パラ数|量子化|サイズ|環境|tps|RAM|VRAM
---:|---|---:|---|---:|---:|---:
8x8B|Q4_K_M|28GB|CPU|5.08|27.6GB|
8x8B|Q4_K_M|28GB|CPU+GPU|6.44|28.7GB|13.2GB

:::message
CPU+GPU での RAM 使用量は、モデル読み込み直後は 15.3GB でした。推論を開始すると増加して表の値になりました。
:::

:::details 出力例
AIの未来は非常に明るく、多岐にわたる分野での応用が期待されています。医療では診断支援や新薬開発、自動運転車では交通安全と効率化、教育では個別学習の支援などがあります。倫理的な課題にも取り組む必要があり、人間と共存しながら進化していくでしょう。

----

AIの未来は非常に明るい展望があります。今後、より高度な自然言語処理や画像認識技術が進化し、人間とのインタラクションがさらにスムーズになるでしょう。また、医療分野では診断支援システムの精度が向上し、個別化治療が可能になります。さらに、教育においても個々の学習者に最適化されたカリキュラムが提供されるようになることが期待されます。倫理的な課題にも対処しつつ、持続可能で効率的な社会の実現に寄与するでしょう。
:::

モデルサイズが大き過ぎるため、VRAM 16GB に収まるレイヤだけ GPU にオフロードされます。

```text
llm_load_tensors: offloading 16 repeating layers to GPU
llm_load_tensors: offloaded 16/33 layers to GPU
```

33 個のレイヤのうち 1 つは出力用で、CPU に割り当てられています。それ以外の 32 個のうち半分の 16 個が GPU にオフロードされます。

このような状況で、速度はフル CPU に比べて約 1.2 倍程度の向上です。

## IQ2_XXS

VRAM に収まるサイズも試します。

```sh
ollama run hf.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GGUF:IQ2_XXS
```

パラ数|量子化|サイズ|環境|tps|RAM|VRAM
---:|---|---:|---|---:|---:|---:
8x8B|IQ2_XXS|12GB|CPU|3.87|12.9GB|
8x8B|IQ2_XXS|12GB|GPU|18.16|13.4GB|13.1GB

:::details 出力例
AI技術がさらに進化し、より人間らしい対話が可能になることで、人と機械の関係性はますます深まり、相互理解や共感のレベルも向上するでしょう。また、個々のニーズに応じたパーソナライズされたサービス提供が進み、ユーザーエクスペリエンスが格段に向上します。その結果として、医療から教育まで様々な分野でAIを活用したソリューションが広く普及し、人々の生活の質が大きく向上すると考えられます。

---

まず、AI技術の進化は今後も続くと予想されます。特に機械学習やディープラーニングの分野では、より高度なアルゴリズムとモデルが開発されるでしょう。これにより、自然言語処理の精度が向上し、ユーザーとの対話がさらにスムーズになると考えられます。

また、倫理的な側面や社会的な影響も重要な課題として取り上げられるようになります。AIの透明性や公平性を確保するための技術開発が進むでしょう。例えば、バイアスを減少させるための新しいアルゴリズムや、プライバシーを保護するためのデータ管理技術などが発展すると考えられます。

さらに、医療分野や教育分野など、さまざまな産業でのAIの応用が広がると予測されます。これにより、人々の生活の質が向上し、社会全体の効率性が向上することが期待されます。

総じて、AI技術は今後も進化し続ける一方で、その利用方法や倫理的側面についても慎重に検討されるでしょう。これにより、AIと人間が共存する未来が描かれることが予想されます。
:::

すべてのレイヤが GPU にオフロードされています。

```text
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
```

計算が複雑になるため CPU での動作は Q4_K_M より遅くなりますが、VRAM に入りきるため GPU ではかなり高速化します。

# まとめ

パラ数|量子化|サイズ|環境|tps|RAM|VRAM
---:|---|---:|---|---:|---:|---:
8B|Q6_K|6.2GB|CPU|6.77|6.8GB|
8B|Q6_K|6.2GB|GPU|37.50|6.2GB|7.9GB
8x8B|Q4_K_M|28GB|CPU|5.08|27.6GB|
8x8B|Q4_K_M|28GB|CPU+GPU|6.44|28.7GB|13.2GB
8x8B|IQ2_XXS|12GB|CPU|3.87|12.9GB|
8x8B|IQ2_XXS|12GB|GPU|18.16|13.4GB|13.1GB

![graph.png](https://storage.googleapis.com/zenn-user-upload/2775907b2827-20250221.png)

MoE はモデル全体がアクティブにならないとは言え、やはり GPU に入り切るかどうかは重要なようです。

この結果は、規模を拡大して DeepSeek を動かす場合にも参考になりそうです。

# 比較

同様の方法で非 MoE の Llama 3.3 70B を測定した記事です。

https://zenn.dev/7shi/articles/c5f1ae59a67462

量子化方式が異なるため条件は異なりますが、MoE の全体がアクティブにならない特性から、ほぼ同じサイズでも動作速度には明確な差があります。

モデル|パラ数|量子化|サイズ|環境|tps|RAM|VRAM
---|---:|---|---:|---|---:|---:|---:
Tanuki-dpo-v1.0|8x8B|Q4_K_M|28GB|CPU|5.08|27.6GB|
Tanuki-dpo-v1.0|8x8B|Q4_K_M|28GB|CPU+GPU|6.44|28.7GB|13.2GB
Llama 3.3|70B|Q2_K|26GB|CPU|1.46|27.2GB|
Llama 3.3|70B|Q2_K|26GB|CPU+GPU|2.42|26.9GB|15.7GB

# 関連記事

同様のプロンプトで計測した記事です。Radeon ドライバについて説明しています。GeForce RTX 4060 Ti との比較があります。

https://qiita.com/7shi/items/dc037c2d5b0add0da33a

Intel Arc A770 との比較があります。

https://7shi.hateblo.jp/entry/2024/12/17/020636

# 追記

zakki さんより、override-tensor 機能が開発中だと教えていただきました。

@[tweet](https://x.com/k_matsuzaki/status/1893519267028742276)
