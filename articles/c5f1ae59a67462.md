---
title: "VRAM 不足の状況で Llama 3.3 70B を動かす"
emoji: "🦙"
type: "tech"
topics:
  - "llm"
  - "llama"
  - "ollama"
published: true
published_at: "2025-02-23 11:20"
---

VRAM に入り切らない状況で Llama 3.3 70B を動かして、CPU 動作と比較しました。

# 環境

以下の環境で測定しました。

- OS: Windows 11 Home [Version 10.0.26100.3194]
- RAM: 64GB (DDR4-3200)
- CPU: AMD Ryzen 5 5600X 6-Core Processor
- GPU: AMD Radeon RX 7600 XT (VRAM 16GB)
- Ollama: 0.5.11

:::message
この Ollama のバージョンでは、VRAM の容量を見て自動的に一部を GPU にオフロードします。
:::

`/set verbose` を指定して、以下のプロンプトを実行しました。

```text
AIの未来を予測してください。（100字）
```

:::message
Llama 3.3 はたまに中国語で回答することがあります。その場合、日本語で回答するまで再試行しました。
:::

# Q4_K_M

```sh
ollama run llama3.3
```

パラ数|量子化|サイズ|環境|tps|RAM|VRAM
---:|---|---:|---|---:|---:|---:
70B|Q4_K_M|42GB|CPU|0.99|42GB|
70B|Q4_K_M|42GB|CPU+GPU|1.34|41.6GB|15.2GB

:::message
RAM/VRAM 列は、測定前との差分で算出したメモリ使用量です。サイズは量子化後のモデルサイズです。tps (tokens per second) は 1 秒あたりの出力トークン数です。
:::

:::details 出力例
AIは、医療や金融などさまざまな分野で活用されます。将来的には、人間とAIの協力により、生産性が向上し、新しい業界が誕生します。また、AIを悪用したサイバー攻撃の増加も懸念されます。安全性と倫理性を確保するための技術開発と規制強化が必要です。

----

AIは、将来的にますます高度化し、人間と密接に連携することになる。AIによる自動運転や医療支援などの実用的な応用が広まる一方で、人工知能の倫理や安全性に関する懸念も深まると予想される。さらに、AIの発展は仕事の形や社会構造に大きな変化をもたらす可能性があるため、教育や労働政策などの対応が必要になる。
:::

モデルサイズが大き過ぎるため、VRAM 16GB に収まるレイヤだけ GPU にオフロードされます。（実際には 2GB ほど共有 GPU メモリにはみ出します）

```text
llm_load_tensors: offloading 27 repeating layers to GPU
llm_load_tensors: offloaded 27/81 layers to GPU
```

81 個のレイヤのうち 1 つは出力用で、CPU に割り当てられています。それ以外の 80 個のうち 27 個が GPU にオフロードされます。

このような状況で、速度はフル CPU に比べて約 1.3 倍程度の向上です。

# Q2_K

Q4_K_M は巨大すぎて速度が出ないため、サイズが小さいものも試します。

```sh
ollama run hf.co/unsloth/Llama-3.3-70B-Instruct-GGUF:Q2_K
```

パラ数|量子化|サイズ|環境|tps|RAM|VRAM
---:|---|---:|---|---:|---:|---:
70B|Q2_K|26GB|CPU|1.46|27.2GB|
70B|Q2_K|26GB|CPU+GPU|2.42|26.9GB|15.7GB

:::details 出力例
AIは人工知能、機械学習、ディープラーニングなどで進化し、人間に匹敵する知能を持つ存在となる可能性が高い。データ解析や自動運転、医療などに幅広い応用が期待されるが、道徳感の喪失や雇用の減少も懸念され、アメリカではAIによる人材搶奪を防ぐための教育の改革が始まっている。

----

人工知能（AI）の将来は、人類に多くの利益をもたらすものになるでしょう。AI技術が発展し、医療や教育、交通などの分野で革命を起こしていくことが予測されます。また、ロボットと人間の共存による新しい社会への移行も期待されています。
:::

モデルサイズが大き過ぎるため、VRAM 16GB に収まるレイヤだけ GPU にオフロードされます。（実際には 2GB ほど共有 GPU メモリにはみ出します）

```text
llm_load_tensors: offloading 46 repeating layers to GPU
llm_load_tensors: offloaded 46/81 layers to GPU
```

81 個のレイヤのうち 1 つは出力用で、CPU に割り当てられています。それ以外の 80 個のうち 46 個が GPU にオフロードされます。

このような状況で、速度はフル CPU に比べて約 1.6 倍程度の向上です。

# まとめ

パラ数|量子化|サイズ|環境|tps|RAM|VRAM
---:|---|---:|---|---:|---:|---:
70B|Q4_K_M|42GB|CPU|0.99|42GB|
70B|Q4_K_M|42GB|CPU+GPU|1.34|41.6GB|15.2GB
70B|Q2_K|26GB|CPU|1.46|27.2GB|
70B|Q2_K|26GB|CPU+GPU|2.42|26.9GB|15.7GB

![graph.png](https://storage.googleapis.com/zenn-user-upload/66f2cfd68cdc-20250223.png)

# 比較

同様の方法で MoE を測定した記事です。

https://zenn.dev/7shi/articles/8c4255d199264f

量子化方式が異なるため条件は異なりますが、MoE の全体がアクティブにならない特性から、ほぼ同じサイズでも動作速度には明確な差があります。

モデル|パラ数|量子化|サイズ|環境|tps|RAM|VRAM
---|---:|---|---:|---|---:|---:|---:
Llama 3.3|70B|Q2_K|26GB|CPU|1.46|27.2GB|
Llama 3.3|70B|Q2_K|26GB|CPU+GPU|2.42|26.9GB|15.7GB
Tanuki-dpo-v1.0|8x8B|Q4_K_M|28GB|CPU|5.08|27.6GB|
Tanuki-dpo-v1.0|8x8B|Q4_K_M|28GB|CPU+GPU|6.44|28.7GB|13.2GB

# 関連記事

同様のプロンプトで計測した記事です。Radeon ドライバについて説明しています。GeForce RTX 4060 Ti との比較があります。

https://qiita.com/7shi/items/dc037c2d5b0add0da33a

Intel Arc A770 との比較があります。

https://7shi.hateblo.jp/entry/2024/12/17/020636

# 追記

はる猫大福(haru_arc)さんより、M2 Ultra で Llama 3.3 70B (Q4_K_M) が VRAM にすべて収まる場合は 12.21 tps 出ると教えていただきました。
