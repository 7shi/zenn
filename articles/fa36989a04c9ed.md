---
title: "OllamaのJSONモードと思考機能の相互作用"
emoji: "🦙"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["ollama", "jsonスキーマ"]
published: true
---

OllamaのPythonライブラリが提供する`chat`関数は、`format`パラメータを通じてモデルの出力形式を制御する機能を備えています。本記事では、このパラメータがどのように機能し、思考（thinking）機能とどう相互作用するのかの調査結果をまとめたものです。

OllamaのPythonライブラリからOllamaサーバー内部のllama.cppまでの処理フローを追跡します。

https://github.com/ollama/ollama-python

https://github.com/ollama/ollama

:::message
本記事は、Gemini CLIとClaude Codeによるソースコードの調査結果をまとめたものです。
:::

## JSONモードと思考機能の排他性

調査における最も重要な発見は、**`format='json'`と`think=True`が技術的な制約により同時に機能しない、排他的な関係にある**という点です。

### 技術的な理由

この排他性には、主に2つの技術的理由があります。

1.  **GBNF文法による制約**:
    JSON出力が指定されると、LLMの出力はGBNF文法によって厳密にJSON構造に制約されます。この文法は`<think>`のようなJSON外のタグを許可しないため、モデルは思考タグを生成すること自体ができません。

2.  **Thinkingパーサーの限界**:
    Ollamaサーバーの思考分離機能[thinking/parser.go:57-74](https://github.com/ollama/ollama/blob/44b17d2bfa0073e012679152421c0b69671d380e/thinking/parser.go#L57)は、生のテキストストリームからタグを抜き出す実装です。JSONの構造を解析して値の中に埋め込まれたタグを抽出する能力はないため、たとえタグが生成されたとしても分離できません。

### 実験結果に見る挙動の違い

この排他的な関係は、実際のレスポンスに明確に現れます。

**`format`を指定せず`think=True`のみを使用した場合**、思考プロセスは正しく分離されます。

```json
{
  "message": {
    "thinking": "ユーザーは日本の首都を尋ねている...",
    "content": "日本の首都は東京です。"
  }
}
```

一方、**`format='json'`と`think=True`を併用した場合**、思考プロセスは`content`フィールド内のJSONの一部として返され、`thinking`フィールドは`null`になります。

```json
{
  "message": {
    "thinking": null,
    "content": "{\"answer\": \"東京\", \"reasoning\": \"思考プロセス...\"}"
  }
}
```

## `format`パラメータの基本仕様

`format`パラメータの型は、[ollama/_types.py:154](https://github.com/ollama/ollama-python/blob/63ca74762284100b2f0ad207bc00fa3d32720fbd/ollama/_types.py#L154)で以下のように定義されています。

```python
format: Optional[Union[Literal['', 'json'], JsonSchemaValue]] = None
```

この定義により、パラメータはフォーマットを指定しない`None`または空文字列、JSON出力を強制する`'json'`、そしてより複雑な構造を定義するための`JsonSchemaValue`（JSONスキーマオブジェクト）を受け付けます。

## クライアントからサーバーへの処理フロー

`format`パラメータがどのように処理されるかを、クライアント側とサーバー側に分けて見ていきましょう。

### 1. クライアント側処理：透過的なパラメータ送信

Pythonライブラリでは、`format`パラメータは特別な変換を受けることなく、そのままサーバーへ送信されます。[ollama/_client.py:346-355](https://github.com/ollama/ollama-python/blob/63ca74762284100b2f0ad207bc00fa3d32720fbd/ollama/_client.py#L346)で`ChatRequest`オブジェクトが構築される際、パラメータは透過的に渡され、Pydanticによる型検証のみが実行されます。この設計により、クライアントはサーバー側の詳細なロジックを意識することなく、安全にリクエストを構築できます。

```python
json=ChatRequest(
  model=model,
  messages=list(_copy_messages(messages)),
  tools=list(_copy_tools(tools)),
  stream=stream,
  think=think,
  format=format,
  options=options,
  keep_alive=keep_alive,
).model_dump(exclude_none=True),
```

### 2. サーバー側処理：GBNF文法への変換

Ollamaサーバーは、メインプロセスと`runner`プロセスから成る**2段階HTTPリクエストアーキテクチャ**を採用しており、APIリクエストの受付と実際のLLM推論を分離しています。

クライアントから`format`パラメータを受け取ったメインサーバーは、[server/routes.go:1539](https://github.com/ollama/ollama/blob/44b17d2bfa0073e012679152421c0b69671d380e/server/routes.go#L1539)のハンドラでリクエストが処理されます。

```go
if err := r.Completion(c.Request.Context(), llm.CompletionRequest{
	Prompt:  prompt,
	Images:  images,
	Format:  req.Format,  // 1539行目
	Options: opts,
}, func(r llm.CompletionResponse) {
```

[llm/server.go:739-759](https://github.com/ollama/ollama/blob/44b17d2bfa0073e012679152421c0b69671d380e/llm/server.go#L739)でその値を解釈します。

```go
if len(req.Format) > 0 {
	switch string(req.Format) {
	case `null`, `""`:
		// Field was set, but "missing" a value. We accept
		// these as "not set".
		break
	case `"json"`:
		req.Grammar = grammarJSON
	default:
		if req.Format[0] != '{' {
			return fmt.Errorf("invalid format: %q; expected \"json\" or a valid JSON Schema object", req.Format)
		}

		// User provided a JSON schema
		g := llama.SchemaToGrammar(req.Format)
		if g == nil {
			return fmt.Errorf("invalid JSON schema in format")
		}
		req.Grammar = string(g)
	}
}
```

- `format`が`'json'`の場合、定義済みのJSON用GBNF文法（`grammarJSON`）が適用されます。
- JSONスキーマが指定された場合は、`llama.SchemaToGrammar`関数によって動的にGBNF文法へ変換されます。

この生成された文法は最終的に`runner`プロセス[runner/llamarunner/runner.go:572](https://github.com/ollama/ollama/blob/44b17d2bfa0073e012679152421c0b69671d380e/runner/llamarunner/runner.go#L572)のサンプリングパラメータに設定され、LLMの出力を直接的かつ厳密に制御します。

```go
// Extract options from the CompletionRequest
samplingParams := llama.SamplingParams{
	TopK:           req.Options.TopK,
	TopP:           req.Options.TopP,
	MinP:           req.Options.MinP,
	TypicalP:       req.Options.TypicalP,
	Temp:           req.Options.Temperature,
	RepeatLastN:    req.Options.RepeatLastN,
	PenaltyRepeat:  req.Options.RepeatPenalty,
	PenaltyFreq:    req.Options.FrequencyPenalty,
	PenaltyPresent: req.Options.PresencePenalty,
	Seed:           uint32(req.Options.Seed),
	Grammar:        req.Grammar,  // 572行目
}
```

## runnerプロセスでのGrammar処理の詳細

runnerに渡されたGrammarパラメータは、以下のようにLLMの出力を制約します。

### 1. サンプリングコンテキストへの受け渡し

[llama/llama.go:550-554](https://github.com/ollama/ollama/blob/44b17d2bfa0073e012679152421c0b69671d380e/llama/llama.go#L550)でGrammar文字列がC++層に渡されます：

```go
grammar := C.CString(params.Grammar)
defer C.free(unsafe.Pointer(grammar))

cparams.grammar = grammar
context := &SamplingContext{c: C.common_sampler_cinit(model.c, &cparams)}
```

### 2. トークンサンプリング時の文法適用

[llama/llama.cpp/common/sampling.cpp:346-348,367,377](https://github.com/ollama/ollama/blob/44b17d2bfa0073e012679152421c0b69671d380e/llama/llama.cpp/common/sampling.cpp#L346)で、各トークン生成時に文法チェックが行われます：

sampling.cpp:346-347行目 - grammar_firstモードの場合、文法を最初に適用

```cpp
if (grammar_first) {
    llama_sampler_apply(grmr, &cur_p);
}
```

sampling.cpp:367行目 - トークンが文法に適合するかチェック

```cpp
const bool is_valid = single_token_data_array.data[0].logit != -INFINITY;
```

sampling.cpp:377行目 - 無効な場合の再サンプリング

```cpp
llama_sampler_apply(grmr,  &cur_p);
```

### 3. 文法による制約の実装

[llama/llama.cpp/src/llama-grammar.cpp](https://github.com/ollama/ollama/blob/44b17d2bfa0073e012679152421c0b69671d380e/llama/llama.cpp/src/llama-grammar.cpp)の`llama_grammar_apply_impl`関数で、以下の3段階の処理が行われます：

1. **各トークンをUTF-8文字列にデコード** (llama-grammar.cpp:1145-1147)：  
```cpp
const std::string piece = grammar.o_vocab ?
    grammar.o_vocab->token_to_piece(id) :
    grammar.vocab->token_to_piece(id);
```

2. **文法ルールに対してトークンを検証** (llama-grammar.cpp:1163)：
```cpp
const auto rejects = llama_grammar_reject_candidates(grammar.rules, grammar.stacks, candidates_grammar);
```

3. **無効なトークンのlogitを負の無限大に設定** (llama-grammar.cpp:1164-1166)：
```cpp
for (const auto & reject : rejects) {
    cur_p->data[reject.index].logit = -INFINITY;
}
```

この仕組みにより、GBNF文法に違反するトークンは物理的に生成不可能となり、JSONモードでは`<think>`タグのような非JSON構造が出力されることはありません。

## アーキテクチャ分析と結論

この一連の動作は、Ollamaの設計思想を反映しています。クライアントからサーバーへパラメータを**透過的**に渡し、Pydanticによる**型安全性**を確保し、GBNF文法を用いてLLMの出力を**誘導的に制約**するアーキテクチャは、非常に洗練されています。

特にGBNF文法の役割は重要です。これは生成されたテキストを後からフィルタリングするのではなく、トークン生成の確率分布そのものを制御し、文法的に無効なトークンの生成確率を実質的にゼロ（-INFINITY）にすることで、出力形式を保証します。

### 結論

1.  **JSONモードは思考分離機能と排他的です。** これはGBNF文法とThinkingパーサーの技術的制約に起因するものであり、両機能を併用したい場合は、JSONスキーマ内に思考内容を格納するフィールドを明示的に定義するアプローチが必要です。

2.  **`format`パラメータはクライアントからサーバーへ透明に伝達されます。** クライアント側では型検証のみが行われ、実際の処理はすべてサーバー側で実行されます。

3.  **エコシステム全体でEnd-to-Endの型安全性が確保されています。** PythonライブラリのPydanticバリデーションから、サーバーのレスポンス復元まで、堅牢な設計が貫かれています。

この調査により、クライアントライブラリの型安全性から、サーバー内部の2段階アーキテクチャ、そしてGBNFによる出力制御に至るまで、Ollamaエコシステム全体の`format`パラメータに関する包括的な理解が得られました。
