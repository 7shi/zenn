---
title: "OllamaのJSONモードと思考機能の相互作用"
emoji: "🦙"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["ollama", "jsonスキーマ"]
published: true
---
OllamaのPythonライブラリが提供する`chat`関数は、`format`パラメータを通じてモデルの出力形式を制御する機能を備えています。本記事では、このパラメータがどのように機能し、思考（thinking）機能とどう相互作用するのかの調査結果をまとめたものです。

OllamaのPythonライブラリからOllamaサーバー内部のllama.cppまでの処理フローを追跡します。

https://github.com/ollama/ollama-python

https://github.com/ollama/ollama

:::message
本記事は、Gemini CLIとClaude Codeによるソースコードの調査結果をまとめたものです。
:::

## `format`パラメータの基本仕様

`format`パラメータの型は、`ollama/_types.py:154`で以下のように定義されています。

https://github.com/ollama/ollama-python/blob/63ca74762284100b2f0ad207bc00fa3d32720fbd/ollama/_types.py#L154

この定義により、パラメータはフォーマットを指定しない`None`または空文字列、JSON出力を強制する`'json'`、そしてより複雑な構造を定義するための`JsonSchemaValue`（JSONスキーマオブジェクト）を受け付けます。

## クライアントからサーバーへの処理フロー

`format`パラメータがどのように処理されるかを、クライアント側とサーバー側に分けて見ていきましょう。

### 1. クライアント側処理：透過的なパラメータ送信

Pythonライブラリでは、`format`パラメータは特別な変換を受けることなく、そのままサーバーへ送信されます。`ollama/_client.py:346-356`で`ChatRequest`オブジェクトが構築される際、パラメータは透過的に渡され、Pydanticによる型検証のみが実行されます。この設計により、クライアントはサーバー側の詳細なロジックを意識することなく、安全にリクエストを構築できます。

https://github.com/ollama/ollama-python/blob/63ca74762284100b2f0ad207bc00fa3d32720fbd/ollama/_client.py#L346

### 2. サーバー側処理：GBNF文法への変換

Ollamaサーバーは、メインプロセスと`runner`プロセスから成る**2段階HTTPリクエストアーキテクチャ**を採用しており、APIリクエストの受付と実際のLLM推論を分離しています。

クライアントから`format`パラメータを受け取ったメインサーバーは、`server/routes.go:1539`のハンドラでリクエストを処理し、`llm/server.go:739-758`でその値を解釈します。

https://github.com/ollama/ollama/blob/44b17d2bfa0073e012679152421c0b69671d380e/server/routes.go#L1539

https://github.com/ollama/ollama/blob/44b17d2bfa0073e012679152421c0b69671d380e/llm/server.go#L739

- `format`が`'json'`の場合、定義済みのJSON用GBNF文法（`grammarJSON`）が適用されます。
- JSONスキーマが指定された場合は、`llama.SchemaToGrammar`関数によって動的にGBNF文法へ変換されます。

この生成された文法は最終的に`runner`プロセス（`runner/llamarunner/runner.go:572`）のサンプリングパラメータに設定され、LLMの出力を直接的かつ厳密に制御します。

https://github.com/ollama/ollama/blob/44b17d2bfa0073e012679152421c0b69671d380e/runner/llamarunner/runner.go#L572

## JSONモードと思考機能の排他性

調査における最も重要な発見は、**`format='json'`と`think=True`が技術的な制約により同時に機能しない、排他的な関係にある**という点です。

### 技術的な理由

この排他性には、主に2つの技術的理由があります。

1.  **GBNF文法による制約**:
    JSON出力が指定されると、LLMの出力はGBNF文法によって厳密にJSON構造に制約されます。この文法は`<think>`のようなJSON外のタグを許可しないため、モデルは思考タグを生成すること自体ができません。

2.  **Thinkingパーサーの限界**:
    Ollamaサーバーの思考分離機能（`thinking/parser.go:57-74`）は、生のテキストストリームからタグを抜き出す実装です。JSONの構造を解析して値の中に埋め込まれたタグを抽出する能力はないため、たとえタグが生成されたとしても分離できません。

https://github.com/ollama/ollama/blob/44b17d2bfa0073e012679152421c0b69671d380e/thinking/parser.go#L57

### 実験結果に見る挙動の違い

この排他的な関係は、実際のレスポンスに明確に現れます。

**`format='json'`と`think=True`を併用した場合**、思考プロセスは`content`フィールド内のJSONの一部として返され、`thinking`フィールドは`null`になります。

```json
{
  "message": {
    "thinking": null,
    "content": "{\"answer\": \"東京\", \"reasoning\": \"思考プロセス...\"}"
  }
}
```

一方、**`format`を指定せず`think=True`のみを使用した場合**、思考プロセスは正しく分離されます。

```json
{
  "message": {
    "thinking": "ユーザーは日本の首都を尋ねている...",
    "content": "日本の首都は東京です。"
  }
}
```

## アーキテクチャ分析と結論

この一連の動作は、Ollamaの堅牢な設計思想を反映しています。クライアントからサーバーへパラメータを**透明**に渡し、Pydanticによる**型安全性**を確保し、GBNF文法を用いてLLMの出力を**誘導的に制約**するアーキテクチャは、非常に洗練されています。

特にGBNF文法の役割は重要です。これは生成されたテキストを後からフィルタリングするのではなく、トークン生成の確率分布そのものを制御し、文法的に無効なトークンの生成確率を実質的にゼロにすることで、出力形式を保証します。

### 結論

1.  **JSONモードは思考分離機能と排他的です。** これはGBNF文法とThinkingパーサーの技術的制約に起因するものであり、両機能を併用したい場合は、JSONスキーマ内に思考内容を格納するフィールドを明示的に定義するアプローチが必要です。

2.  **`format`パラメータはクライアントからサーバーへ透明に伝達されます。** クライアント側では型検証のみが行われ、実際の処理はすべてサーバー側で実行されます。

3.  **エコシステム全体でEnd-to-Endの型安全性が確保されています。** PythonライブラリのPydanticバリデーションから、サーバーのレスポンス復元まで、堅牢な設計が貫かれています。

この調査により、クライアントライブラリの型安全性から、サーバー内部の2段階アーキテクチャ、そしてGBNFによる出力制御に至るまで、Ollamaエコシステム全体の`format`パラメータに関する包括的な理解が得られました。
